
## Who's Aligning Who?

In the weeks to date, we have covered:

 - Synthesis
 - Experience
 - Recognition
 - Attention
 - Consciousness

And so far we are developing - in a dialectical fashion! - a sense of machine learning and human learning. We see what these two modalities share in common; and (perhaps!) what distinguishes them.

In the remaining three weeks we will look more closely at the *relationship* between these two modes. We will first begin with the question of *alignment*. 

Typically this term is taken in some general sense of aligning machines to humans. Why is this a problem? Let's look at an abstract scenario: we want data to train a language model of our own. We know we need a lot of data, so we scrape the data of the Internet (just as a search engine might do). We then use the data to train our model (using the Transformer model and attention mechanism we learned in Week 4). 

Just like the "Attention is All You Need" paper suggested, we get a model that generates meaningful text to a whole range of questions we ask it: on anything from cinema trivia to mathematical formulae. Congrats! We unleash our new model on the world... What could go wrong?

Soon social media is ablaze with criticism: our UIUC LDL program app is full of contradictions. It doesn't know when to stop. It lies flagrantly, including about the fact that it lies. It can be trivially tricked into saying things which are offensive to different groups. It does not reflect – we have to admit it – our *values*.

So what do we do? We realize our bot has taken on a life of its own, and need to be harnessed to our values. In the language of AI research, it must be *aligned*. It turns out unfortanately attention is not all we need...

In the readings this week, we review the seminal paper that introduced alignment (Ouyang et al., 2022). This is the paper that really paved the way for moving from GPT-3 – a capable but problematic model, released in 2020 – to the service that could be mainstreamed – ChatGPT, released in November 2022.

We are also including two example of more recent literature. The first, by Shen et al. (2024), acknowledges the reverse situation: humans becoming aligned with AI. As AI becomes a default technology for young learners particularly, we might need to think how machine learning has begun to precede and determine human learning. The second, by Greenblatt et al. (2024), comes from Anthropic, and illustrates the intriguing - if also disturbing – possibility of "alignment faking". 

Finally – and excuse the self-referencing - I have also included three papers that I have developed. As with the technical papers above, I don't suggest reading these cover to cover, but instead just glancing through and taking note of how alignment features in all three: in relation to bias; in relation to different conceptions of truth; and as a problem that resembles the alignment of deviant human subjects. 


### Readings

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730–27744. https://proceedings.neurips.cc/paper_files/paper/2022/hash/1e4d9a0c36521ee9d62f6c9c3b9dbcff-Abstract-Conference.html

Shen, H., Knearem, T., Ghosh, R., Alkiek, K., Krishna, K., Liu, Y., Ma, Z., Petridis, S., Peng, Y.-H., Qiwei, L., Rakshit, S., Si, C., Xie, Y., Bigham, J. P., Bentley, F., Chai, J., Lipton, Z., Mei, Q., Mihalcea, R., Terry, M., Yang, D., Morris, M. R., Resnick, P., & Jurgens, D. (2024). Towards bidirectional human-AI alignment: A systematic review for clarifications, framework, and future directions. arXiv. https://doi.org/10.48550/arXiv.2406.09264

Greenblatt, R., Denison, C., Wright, B., Roger, F., MacDiarmid, M., Marks, S., Treutlein, J., Belonax, T., Chen, J., Duvenaud, D., Khan, A., Michael, J., Mindermann, S., Perez, E., Petrini, L., Uesato, J., Kaplan, J., Shlegeris, B., Bowman, S. R., & Hubinger, E. (2024). Alignment faking in large language models. arXiv. https://doi.org/10.48550/arXiv.2412.14093


Magee, L., Ghahremanlou, L., Soldatic, K., & Robertson, S. (2021). Intersectional bias in causal language models. arXiv preprint arXiv:2107.07691. https://arxiv.org/abs/2107.07691

Munn, L., Magee, L., & Arora, V. (2024). Truth machines: synthesizing veracity in AI language models. AI & society, 39(6), 2759-2773. https://link.springer.com/article/10.1007/s00146-023-01756-4

Hristova, T., Magee, L., & Soldatic, K. (2025). The problem of alignment. AI & SOCIETY, 40(3), 1439-1453. https://link.springer.com/article/10.1007/s00146-024-02039-2.


### Questions

- **Who Is Actually Aligning Who?** We have covered aspects of both machine and human alignment this week. With rising concern about the issues like cognitive lock-in, "alignment" is a process that already goes both ways: human to machine, and machine to humans. Do we trust machines to convey and mediate transfer of human values? In what ways does AI differ from "everyday" transfer of ideas (including dangerous ones) via books and other media? 
- **Normalcy and Deviance**. In some ways, as one of the readings suggests, machine alignment parallels the troubled history of aligning deviance to social norms – a major concern for, among others, French philosopher Michel Foucault. Do you agree? If so, what questions does this raise about education as a grand institutional alignment exercise? If not, why not - and can society learn anything from computational alignment processes (e.g. for prison reform, rehabilitation, tolerance)?
- **To Align or Not to Align?** Last week some of you suggested machines might arrive at their own distinct – and for us, unknowable – form of consciousness. Does alignment interfere or instead accelerate this distinctiveness? Might we see a day (or have we already) when machines only *pretend* to align?





### More Details on the Final Assessment

By the end of this course we'll have covered a number of key concepts relating to machine and human learning:

 - Synthesis
 - Experience
 - Recognition
 - Attention
 - Consciousness
 - Alignment
 - Critique
 - Synthesis (again)

The final assignment aims to pull together the concepts and associated readings and discussions in a format you might (hopefully) carry across to your practice and further research. As a piece of summative assessment, we would like you to develop one of the following, comprising approximately 1,500 to 2,000 words:

 - Curriculum outline
 - Handbook
 - Learning guide
 - Conference presentation

The topic should be one of the following:

  a. Learning materials **designed for machines**, describing some facet of human culture
  b. Learning materials **designed for humans**, describing some facet of machine "experience"
  c. A hybrid learning guide for a **new world of cyber-social learning** - how both machines and humans can learn from each other.

Other related topics and formats are permissable, but please check with either Liam or Michele first. 

We encourage you to integrate concepts from the course into these learning materials. How, for example, would you train a machine to "recognize" a human, in the rich and existential sense that Hegel means? How would you describe for a human the technical and phenomenological sense of what it means for a machine to "attend" to words in a stream of input?

While the **text** must be your own, we would also encourage you to use AI systems to develop diagrams, images or fragments of chat dialogue in support of your work. 