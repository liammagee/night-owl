# Casey Notes



 - Translation to PhD dissertation (parts of general / special field; methodology; chapter 4 - findings)
 - Publish!
 - QUESTION: Scaffolding:
   - Chapters
   - Section breaks
   - Intros / conclusions
   - Watch for acronyms
     - Competency-Based Education (CBE) and Mastery Learning
the methodology for this ERP proceeded through three key phases: (1) Developing assessment items aligned with a structured cryptography concept taxonomy; (2) Conducting a simulated expert review using a GAI model; and (3) Analyzing quantitative data to identify high-performing items 
 - QUESTION: More explicit information about UX design and relevance
 - QUESTION: Limitations of study overall

 - QUESTION: **Problem** of high specialist level - be more explicit
     - Provide note on cybersecurity threats: what are the trends? Threat landscape / attack surface. 
         - Billions of devices
         - Default credentials
         - LLM: more realistic lures. Awareness training.
             - Offence / defence. AI cannot be leverage AI.
     - Current training models not working - filling the demand. 
 - Methodological refinements (as acknowledged):
   - Multiple models (GPT / other)
       - QUESTION: models reviewing other models
   - Machine vs human response
   - QUESTION: Iterated model feedback (drama machine - more recent approaches) - iterated review / refinement
   - QUESTION: Pilot study? What would the control group be?
 - QUESTION: Need high level statements about methods 
 - QUESTION: Generalizability


 - Limitations
 
Cryptography
 - demand for cybersecurity work (WSU - $26 million)
 - workforce development programs alignment with real world demand
 - aging population: open jobs - high end. Pipeline problem ("tip of the spear")
  - Lack of evidence-based practices
  - One-size fits all. Drinking by the firehose
    - Clay Christensen
 - 9 million IT workers - existing knowledge
    - Re-skill / up-skilling cybersecurity (provide 

 - Competency-based Mastery Learning (Bloom) 
     - Not enough content at scale
     - What they know / don't know - next set of instructional materials
 
 - How can GenAI generate & validate items?
 - Simulate subject matter experts

 - Diagnostic Assessments
     - Readiness Assessment - what you know / don't
     - How to design 
     - Computer Adaptive Testing (CAT) - learner  
     - Validty 
     - Ontologies / taxonomies / knowledge graphs
     - Learning Analytics
     - Instructional Design Principles
         - Feedback mechanisms / 
     - Evidence-Centred Design
     - IRT / assessment item types: quality
     - Prompt engineering / role-based prompting / AI-assisted item review


 - Ed Psych
 - Psych / Statistics

 - Concept Matrix
   - 1300 concepts
   - mapped to modules / chapters - validated by 50 experts
   - Focus on cryptography

 - Refined Cognitive Leveling
   - Concept mapping - schemas / mental models
   - QUESTION: Mind mapping - indirection? Extraction from docs? 

 - QUESTION: More on question *types*. Why no typing?
   - Single-select / Multiple-select / Drag and Drop / Dropdown-in-Context / Ranking-Ordering
     - QUESTION: Ranking-Ordering - why no good?

 - Most questions: remember / understand - multiple choice
 - Additional item types - cognitive levels


Role-play: Markdown
 - QUESTION: Too much agreement between reviewers
 - QUESTION: Dimension overlapping - too much agrement? framing / ceiling effect?
 - Cognitive demand vs other three dimensions
 - QUESTION: non-expert role-play?
 - No APIs / temperature models
   - Open access models
   - Multi-model consensus
   - Thematic coding of expert reviewer
   - Test harness
 

### Xinran

 - Gaps: Cybersecurity; Too much work - existing studies for LLM as expert reviewer / qual coding / survey development. Build on existing research.
 - Methodology: AI - how to develop the role-play prompt. Prompt engineering techniques. QUESTION: Inter-rater reliability is a problem.  


### Bill

 - New tech onto psychometrics. von Daviers: gen AI - item generation. 
 - Survey psychometrics. Is this a system to measure what you remember vs what you know? 
   - complex epistemic performance

