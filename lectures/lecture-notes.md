# Lecture Notes

sympoeisis - symbiosis: human / machine learning

whole - part relationship

token economy

divergent / selective learning (GPT-5)


Claude Code:

 - hallucinatory responses > "learn" from user frustration

 - Human - quality assurance. What learning happens? What prior learning is needed?

 - Quasi-empathy for human experience. Polite but ultimately alien

  - Software as a tool for thinking 


mental illness follows its host

 - stochastic society



[[2+2=5]]


 https://chatgpt.com/share/689c83f1-9978-8007-8547-ee46da3aa12f


### Notes


- Dual channels (how humans & machines learn; how they learn together).



- Framing question: What would a genuinely mutual “synthesis” between human learners and machine learners look like, beyond simple tool-use or replacement?- Clarify terms: “Synthesis” here should be read in a Hegelian key as Aufhebung (sublation): a movement that cancels, preserves, and elevates. Not fusion, not compromise—unity-in-difference.- Human learning as Erfahrung (experience): transformation through error, negation, and revision. Learning changes the learner’s standpoint.- ML learning as optimization under loss: error-driven updates (the “labor of the negative”) that reshape the model’s internal representation.- Key analogy: both learn through negation—misclassification or contradiction surfaces what must change; progress requires encountering limits.- Synthesis claim: Human and machine learning can form a single learning circuit where each becomes the other’s condition of improvement.- Mediation map: data, prompts, interfaces, labels, norms, institutions are the concrete mediations that make the circuit possible; synthesis is always mediated, never immediate.- Asymmetries to preserve: embodiment and stakes (humans are accountable, machines are not), normativity (humans legislate ends), opacity forms (human tacit know-how vs. model latent space).- Unity to cultivate: iterative co-adaptation—humans refine tasks, curricula, and norms; models surface patterns, counterexamples, and affordances that reshape human practice.- Recognition lens: RLHF and feedback pipelines as a technical analogue of mutual recognition—models “learn” our norms; we also learn what our norms actually are in practice.- Bildung vs training: human formation involves self-relation and world-relation; design ML pedagogy to support human Bildung (reflection, explainability, failure legibility), not only model accuracy.- Error as teacher: curate workflows where model errors are intelligible enough to educate the human (e.g., uncertainty estimates, counterfactuals), turning failure into shared experience.- Curriculum design: stage tasks dialectically—start with naive use, confront breakdowns, introduce mediations (tools, concepts), then re-integrate with a higher-level practice.- Instrument vs appendage: move from “assistant” metaphor to “appendage/supplement”—externalized cognition that extends but also reshapes the human learner.- Ethical stakes: whose data, whose loss? Synthesis without attention to power reproduces bias; include checkpoints for contestation and revision of objectives.- Practical heuristics for synthesis:  - Keep differences legible (model transparency, provenance).  - Build reversible loops (humans can override, audit, and rollback).  - Expose mediations (make labeling, prompting, and evaluation visible).  - Reward meta-learning (both human reflection and model uncertainty calibration).- Case vignette to discuss: a writing workflow where the human drafts, the model proposes structures, the human critiques and re-prompts, and the model adapts—trace the moments of negation and sublation.- Guiding questions:  - Where exactly does transformation occur—in the human, the model, or the workflow?  - Which mediations are invisible, and how can we surface them?  - What must remain distinct for synthesis to be fruitful rather than absorptive?- Learning outcomes: students can define synthesis as Aufhebung, map mediations in a human–ML loop, and propose one concrete design change that deepens unity-in-difference.