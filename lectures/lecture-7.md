

## Bitter Lessons, Stochastic Parrots, Errant Agents: From Criticism to Immanent Critique?


We have traversed a lot of conceptual territory in this course, and this week we move to another term: critique. 

We will start with a more self-evident term: *criticism*. In the first two readings, we see criticism applied to AI from the standpoint of scholars working in the field of what has been termed "critical AI studies". 

The article by Goodlad and Stone is an introduction to a recent special issue launched by a new journal, in fact entitled "Critical AI". It develops a wide-ranging and lengthy but useful array of critical AI topics. I'll summarize several of these topics here:

---

The article by Gebru and Torres develops criticism in other direction: what I would call an effort to characterize the "spirit" of Silicon Valley, which today motivates the development of Artificial Intelligence. Their description of TESCREAL seeks to join together otherwise disparate and even contradictory elements. On the one hand, the pursuit of scientific, mathematical and technological progress. On the other, the fantasy – at least for today – of an overcoming of mortality and the limits of life, which fall under the banner of transhumanism. 

---

In the follow-up two videos, featuring Yann LeCun and Richard Sutton, we see a different type of criticism. Both are highly respected scholars in the field of Artificial Intelligence, responsible for some of the foundational developments in neural networks and reinforcement learning. Both are however critical about the overreliance upon the Transformer architecture we discussed in Week 4. In different ways, each thinks language models can never arrive at general - much less super - intelligence, because they are inherently limited. 

Both offer criticisms that overlap, but also have differences, certainly in how each sees these limitations being overcome. For LeCun, this limitation relates to the relatively *static* nature of the language model. 

---

For Sutton, the limits of language models relate instead to the lack of *experience*.


---

So far we have examined two examples of two types of criticism. The first we can describe as *external*: developed by well-informed scholars who sit, like most of us, outside the immediate field of machine learning. The second we can describe as *internal*: developed by experts in machine learning itself, who are nonetheless critical of the way the field has evolved. 

---

Now we turn to the fifth article by Stahl, "what is immanent critique?" What *is* immanent critique? And what is its relevance for us, to think about machine and human learning?

Let's start by introducing a couple of further terms: the *object* and the *standpoint* of critique. The object is the thing that we wish to judge with our criticism, while the standpoint is the position we adopt, implicitly or explicitly, when we do so. For example, the Goodlad and Stone article is  directed toward the *object* of artificial intelligence, and their *standpoint* is that technologies such as AI should be equitable, sustainable and fair with respect to the work of creators and workers. 


We'll next introduce another term: the opposite of immanent critique, as a way for us to get into what can be quite a difficult idea to grasp, through the form of contrast. It is a term known as *transcendental* critique. What is transcendental, and how does it differ from immanent? 

Both ideas come in fact from the world of religion. A transcendental god is one who sits outside the world. Who perhaps gives the world its start, via an act of divine creation, but thereafter remains aloof, watching from afar, perhaps only returning at some future date of Judgment. An immanent god, by contrast, is one who is embedded in our world, who is able to act upon it, or who is infused in every act of creation. We have already seen how Hegel's view is deeply immanent: the Spirit is always in a process of maturing, developing, evolving. This includes the Spirit in the godly sense - he means something like god is itself coming into being through the creation of the world, of us, of history, of technology. God's immanence means, for Hegel, that we are not only part of god's plan; our development is also the development of god, or of spirit. Hegel is in some sense of the term, a transhumanist. 

So what about critique? Let's stay with Hegel's idea of immanence a moment longer, even if we treat it just as a convenient fiction. If we assume our history - the history of our species, our culture, our technology – is the coming into being of god, then we might also assume there are missteps. These might be moments when god's development is arrested, held up, because we stray from the proper path. How do we know this? We might believe god's ultimate plan is for the realization of Absolute Knowledge; yet we might feel we are instead losing knowledge, becoming less intelligent, less educated - straying from the tendency towards god's own eventual realization. 

In more concrete terms: we might believe our culture, science and technology *should* be leading us towards greater equality, mutual recognition, self-consciousness and awareness, political harmony, sustainability and so on. But then we notice in practice the opposite is true: wars continue to be fought, xenophobia – a kind of lack of recogition - is on the rise, and so on. 

In short, we notice a contradiction between what *ought* to be the case, and what *is* actually the case. And this leads us to criticism.

Now what makes this criticism *immanent*? In our story so far, we might note we do not in fact have ten commandments or a set of equivalent rules laid out for us. Instead, it is through our own evolution that we develop a sense of right and wrong, or the standards that allow us to talk about what *ought* to be the case. This sense that our standards of judgment arise historically – via our own trial and error - is fundamental to Hegel's picture, and part of his legacy is precisely this idea of critique that is immanent to our own historical process – and to not some *deus ex machina* who presides over us. 

Why does this matter? One of the problems Hegel is trying to address - just like Kant before him – is that of *dogmatism*. Dogmatic beliefs are those that cannot be challenged, because in some way they refer to a transcendental conditions: for example, to a god that *I* believe in, but that *you* do not. Hegel wants to say that dogmatic knowledge, even when it looks like it is true, still rests upon what might be subjective and even irrational beliefs. Such knowledge is itself therefore ultimately irrational. 

Rational knowledge, on the contrary, needs to be based upon foundations that understand themselves to be historically contingent - even if we don't know of any better. Then if we see contradictions, these foundations are capable of being revised - this is the sign of a properly rational knowledge, and the basis for immanent critique. 

Why is this important? In the final analysis, transcendental critique - the kind of criticism that depends upon standards brought from outside the object – only holds if the speaker and the listener share some *a priori* values. For example liberalism: if we are both liberals, your critique of how AI (for example) offends liberal sensibility is likely to resonate. But what if I am opposed to liberalism? We are stuck: you appear to me as dogmatic; I appear to you as sceptical. What breaks this impasse? If you can show me how we are together by a historical process to understand certain values, such as equality, as precious - not as god-given, but rather acquired via painful historical trial-and-error. This doesn't require us both to be atheist; only that we see how history delivers us a set of ideals that seem to improve upon earlier versions. Then you might lead me to a conviction that AI (or something else) contradicts this historical and ethical achievement. Pointing out such a contradiction leads to a criticism that is *immanent*, because it does not depend us having some shared metaphysical values - only that we both acknowledge this historical process (a separate problem).

How do we bring this all home in the context of AI? 





 - Persuasion. Immanent critique: leads to *contradiction*. How do we change our mind? Someone else's mind?

<!--
We might begin by asking the obvious question: how is critique different from criticism? Often the terms are used interchangably. 

But as I suggested in the write-up for this week, Hegel's predecessor Immanuel Kant really defined the term for us. For Kant, critique is the development of an argument about what can and cannot be said about a topic. The Critique of Pure Reason determines what can be known by science, and what cannot. The Critique of Practical Reason, on the other hand, describes how we can know what is moral and what is not - how we ought to regulate our conduct in practical day-to-day life. Finally, the Critique of Judgment describes the conditions under which we apply judgments about beauty. In short, the three Critiques describe how we determine what is true, what is good, and what is beautiful. 

Critique in this sense acts like meta-criticism; it establishes the ground by which criticism - 'that is false / evil / ugly' -  can be applied. How does this relate to our contemporary situation with respect to machine learning? First, I would say much of the discourse around AI today remains locked into *criticism*: relatively little has sought to explicate what constitute the limits of what a machine can learn, or, just as importantly, what machine learning means for the limits of human learning. 

Kant presumes a relatively static world, made up of space, time, and many other things, including things like us, capable of asking questions about that same world. With Hegel, we get a similar picture, but injected now with a dynamic sense of how we as humans can conceive of that world. For Hegel, there are no fixed categories. Instead we work our way through the categories we inherit, discarding some, modifying others, as we test the sum of our knowledge against how we take the world to be. In this very specific sense, Hegel is the first proper philosopher of **technology**: because he allows for the possibility that, alongside and as part of our collective scientific adventure, instruments and machines could revise our understanding of the world quite fundamentally.

Hence Hegel would not have necessarily shocked by profound scientific discoveries and technological developments after his time: Darwin, Freud, Einstein, computers, or machine learning. Instead he would see these as proving his point against Kant - critique must always absorb into itself new 'moments' of discovery. 

Today though, I'm not sure we have quite the same generosity and openness to the prospects of a technology that, as we saw last week, is asking us to align ourselves with the new possibilities it establishes.
-->





<!-- Changing relationship between human and machine. "Assistant" is the wrong metaphor: assumes anthropomorphic "other". Why not a new appendage? Why not acknowledge AI is a powerful cyborgian supplement - like writing, and other technologies?
 -->


<!--


First, a little background: Hegel, a guiding light for this course, was following closely in the footsteps of another German philosopher, Immanuel Kant. Kant introduced the concept of critique in three major works:

 - Critique of Pure Reason (what we today would call, roughly, "Science")
 - Critique of Pratical Reason (what we would call "morality" and "ethics")
 - Critique of Judgment (what we would call "aesthetics", and also art, literature, film studies etc)

"Critique" was not simply meant as a form of criticism, but rather a radical re-thinking of the possibilities of knowledge. For Kant, critique avoids the twin problems of sedimented knowledge: dogmatism (insisting you know what is right) and scepticism (being doubtful that anything is right).

Hegel picks up on Kant's concept of critique, offering (as we might suspect) a *critique* of critique. Where Kant sought to identify limits of knowledge, Hegel saw these limits as themselves unnecessarily arbitrary. Hence the *Phenomenology*'s long road toward Absolute Knowledge.

In relation to AI, we are faced in many respects with similar terms of reference today. On the one hand: those who see AI as the path to unlimited knowledge (the dogmatists). On the other: those who deny that path entirely (the sceptics). And then those who, like Kant and Hegel, seek to describe the very map and contours of what is knowable by machines at all. Are they simply powerful statistical engines? Or potential oracles that, given time, might overcome limits of their training data?

Quite aside from these abstract concerns, there is a fast-growing literature on the negative social effects of AI. We saw some of this already last week in the discussion of alignment. But not all criticism sees alignment as a fix; indeed, for many, the issues are more profound and structural, relating to who *owns* the models, the compute power, the data centres, and the potential means for social surveillance and cognitive lock-in. Even if we remain unconcerned about *doomer* scenarios, many are worried about the intense concentration of resources – including energy, water and real estate, as well as data, computation and human resources – involved in AI today. 

This week we review a small sample of this literature, to gain a sense of how machine learning is far from a political neutral technological development. Yet might there be a sense that the machines are learning from this criticism too?

This week I've added a number of papers that touch upon both critique and criticism of AI. As with previous weeks, pick readings that might interest you and absorb what you can. 

-->



