Now, one of the questions that could naturally arise in teaching a course on machine and human learning, or even just human learning, or even just machine learning, would be, why do we need to go and study a German philosopher from 200 years ago from the early part of the. 19th century. What do we gain by going backwards? And the argument I would want to put, even at this early stage of a course, is that we have to go backwards in order to go forwards, by which I mean there are aspects to this particular philosophical thinker and to the tradition that he belongs to and certainly contributes greatly to that help us both think about human and machine learning, but also to understand, I think, some of the critical differences between them. In short, what I want to argue throughout this course is that the era of machine learning we're in is ironically pre not incentry. and hopefully what I mean, what I'm saying will become clear as we go through. And that argument really relates to the foundational ways of thinking about knowledge and learning, as a consequence, that underpin the entire sort of machine learning history and tradition. Certainly the parts of that tradition that we will be looking at in the context of the machine learning today, and which I would also argue, no matter how many versions of GPT we go through, whether it's 1, two, three, five, or, you know, six through to 10, unless there is a kind of fundamental rethinking of the epistemic conditions of that form of learning, by which I don't just mean sort of technical tricks and algorithmic behhaps hacks to overcome problems like elucination. We will be in a certain sense stuck with machines that learn, but only learn in a certain way, that learn, in a way, there's analogous to human learning, but only to a limited understanding of human learning. So what I want to argue is that there is a particular shift from the time of a manual Khan, mid 18th century German philosopher who very influential on Hegel, some of Hegel's contemporaries, and then Hegel himself. There's a shift within that period that is of monumental significance for thinking about what happens when humans learn. And by extension, not simply humans' thought as individualsual subjects who go through a process of childhood, adolescence, maturity, and so on, but of society. Collectives of individalsuals who also can be said to learn in a different sense. and which we might also say, the era of artificial intelligence machine learning belongs to, that is, it's fundamentally, I would want to argue, a social process that has led to the point at which we have machines that in some beneficing alone at all. So, in a sense, this entire course is structured in a sort of back to the future kind of, according to a back to the future, kind of logic, that we need to go backwards in order to move forwards. And that this particular regression itself, very ironically, is something that has a kind of air of Heelianism about it as well. That is, we have to go backwards in order to sort of negate the present in order to synthesize that negation, that is going back to the past with the present, in order to move properly into the future. Now, of course, the future will come whether we do any of this work or not. It will be a succession of time. But when we think of futurity, think of a purpose of history and by extension, the kind of purpose of education that we are trying to inculcate present generationsations through in order to arrive at some better future world, then we necessarily kind of have to think about what steps we need to take. We need to develop approaches, if you'd like, to thinking about the present critically in terms of a world that is in order to realize that ought to be. So Haigel is really kind of integral to all of this sort of talk of the purpose of history. And you could even say he's the first philosopher that, properly speaking within the Western tradition, theorizes history as a philosophical topic. We're not going to spend, however, too much time on that. And for people interested in that, there's many useful resources which are link to. What is more significant is that this general argument I've been making is that Hegel is the kind of culmination in a certain way of a repudiation of what can be thought as of a naive puricism. And I want to say that much of our approach to thinking about machine learning today still exists or belongs to an era of naive empiricism, specifically that within a fairly simple kind of architecture, we can simply train a model over and over with more computing resources and more time and more data, to the point that it arrives at some kind of simulation of human intelligence. And that that's what we mean when we talk about training and learning educational intelligence, in artificial sense. However, Hagel, in particular represents a very strong critique of that tradition, if that way way of thinking that certainly existed well and truly in Y or origin. And what Haecl wanted to argue is that learning depends upon experience, and experience in turn relies upon arriving at certain points. It's not, in other words, simply an accumulation of data, that that there are qualitative changes along the way of, if you like, pure quantitative heaping of experiences. One way we can think about that is to say that if we think about the individual human individual, we acknowledge that they go through various stages, various periods of development. They don't, in other words, simply start to learn and acquire fact after fact until they arrive at some kind of adult point of maturity and learning. That is no doubt part of the process, but also quite critically, there are processes of self critique, of reexamination, of repudiation, of the very process of learning that it existed up to today. If we think back to our own experiences as adolescence, and certainly when we observe other adolescents we might teach today, we can really see this process of work, I think, in a sort of phenomenological or experiential kind of sense. We might think of students, for example, who are embarrassed about the sort of person they were the year before. So, for example, you might see a year 10 student who is, you remind them of some humorous incident that happened or something they said, one or two years before, and they kind of cringe as though something that they said, you know, it wasn't quite themselves, that that belonged to a different era of themselves, not just the self that didn't have as many facts at their disposal, but an actual different shape of themselves. And this idea of a shape or form, of self, a subjectivity of consciousness, is fundamentally a Hegel Hegelian idea and a key really to understanding, particularly the phenomenology of mind. That really the phenomenology of mind or spirit is the working through different shapes of consciousness, as though each shape can only be arrived at through a shape prior to it that is in some sense also negated and eventually uplifted. talk about this strange German termal, Horn later on, which refers to both simultaneously in negation and an uplifting. It's an example of what's commonly called a contronym, a word that means one thing as well as the exact opposite. And we have some examples in English of contronyms like sanction, which means I permit you to do something, or I forbid you to do something. I impose sanctions, or I sanction you in the sense of allow you to do something. How fabling is a similar kind of word to this, it means negation, cancellation, but also uplifting, transcendence almost of a thing. And if we think of that in terms of human development, well, there are very often periods that we in some sense sense negated, and also then arrive at some kind of enlightenment or period of recognition that comes through that negation. At any rate, the insight that we want to take away from this kind of thinking about Hagle in the context of machine learning is that at the moment, at the moment, the way machines learn, is through accumulation, quantitative accumulation, that, to be sure, arrives at implicit, qualitative differentiations, that we can see that a model, QPT 5 does things differently to GPT 1, are in a qualitatively different way, but it doesn't itself experience. It doesn't experience that shift from one form of one shape of consciousness, one shape of being to another. OK, of course, what I'm saying here, there are a whole lot of unexplained explainers, terms which we might want to unpack if we were to think this through thoroughly, and ideas, for example, like what is consciousness and do machines have consciousness? Could they ever have consciousness? I want to bracket those concerns and considerations for a moment. Certainly in some sense, I think we can say that until there is a machinic consciousness, we could say that there is never going to be learning in the human sense, because humans not only learn, but they learn that they learn, they have self reflexive kind of awareness consciousness of their learning and metognition at work. Machines do not have this as simply not conscious at this point. However, we might want to say that there's some possibility to hold out for the prospect of some kind of, if not for consciousness, then some kind of recognition of a state change that happens through passing from one shape to another. I think that denial of consciousness is not necessarily a denial of some form of change of shape of something, if not consciousness of something at any rate, that marks transitions. If if nothing else in moment could say that the versions of CPT represent something like that, an unself conscious, unself recognizing shift in shape that transpires with the accumulation of data as well.