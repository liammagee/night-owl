@article{MageeHegelsLast,
  title = {Hegel's Last Computer},
  author = {Magee, Liam}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{Lacan2011seminarJacques,
  title = {The seminar of Jacques Lacan: Book XVI: From an Other to the other: 1968-1969.},
  author = {Jacques Lacan},
  year = {2011},
  url = {https://esource.dbs.ie/bitstreams/c4e61ea4-adbd-4a00-876c-f7ee6b13ae93/download}
}

@article{Derrida1981Dissemination,
  title = {Dissemination},
  author = {Jacques Derrida},
  year = {1981},
  url = {https://philpapers.org/rec/JOHD-10}
}

@book{Toscano2023LateFascism,
  title = {Late Fascism: Race, Capitalism and the Politics of Crisis},
  author = {Alberto Toscano},
  year = {2023},
  publisher = {Verso Books},
  url = {https://books.google.com.au/books?hl=en&lr=&id=QbC_EAAAQBAJ&oi=fnd&pg=PR9&dq=toscano+late+fascism&ots=fqxJVKygzY&sig=YJV43bUhyBBjD-LUCMwWPNN3fBY}
}

@article{Magee2023Structuredlike,
  title = {Structured like a language model: Analysing AI as an automated subject},
  author = {Liam Magee, Vanicka Arora, Luke Munn},
  year = {2023},
  journal = {Big Data & Society},
  volume = {10},
  number = {2},
  pages = {20539517231210273},
  doi = {10.1177/20539517231210273},
  url = {http://journals.sagepub.com/doi/10.1177/20539517231210273},
  abstract = {Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as ‘automated subjects’. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise ‘state-of-the-art’ natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer , its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems.}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://journals.sagepub.com/doi/pdf/10.1177/20539517231210273}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@inproceedings{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@article{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Language Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {http://arxiv.org/abs/2407.18219},
  abstract = {A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.}
}

@inproceedings{QuRecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@inproceedings{Qu2024Recursiveintrospection,
  title = {Recursive introspection: Teaching LLM agents how to self-improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=UPoQqreegH}
}

@article{Preprint,
  title = {Preprint PDF},
  url = {http://arxiv.org/pdf/2407.18219v2}
}

@article{Snapshot,
  title = {Snapshot},
  url = {http://arxiv.org/abs/2407.18219}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=UPoQqreegH}
}

@book{Daub2020Whattech,
  title = {What tech calls thinking: An inquiry into the intellectual bedrock of Silicon Valley},
  author = {Adrian Daub},
  year = {2020},
  publisher = {FSG Originals},
  url = {https://books.google.com.au/books?hl=en&lr=&id=IzDQDwAAQBAJ&oi=fnd&pg=PT86&dq=adrian+daub&ots=bVb23NLOZt&sig=pgyW9WDOH_ZkpLWbmqXFqxUp7jo}
}

@article{Linnemann2024ArtificialIntelligence,
  title = {Artificial Intelligence as a New Field of Activity for Applied Social Psychology–A Reasoning for Broadening the Scope},
  author = {Gesa Alena Linnemann, Linda-Elisabeth Reimann},
  year = {2024},
  url = {https://www.preprints.org/manuscript/202406.1586}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://www.preprints.org/manuscript/202406.1586/download/final_file}
}

@article{Weizenbaum1966ELIZAacomputer,
  title = {ELIZA—a computer program for the study of natural language communication between man and machine},
  author = {Joseph Weizenbaum},
  year = {1966},
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36-45},
  doi = {10.1145/365153.365168},
  url = {https://dl.acm.org/doi/10.1145/365153.365168}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://dl.acm.org/doi/pdf/10.1145/365153.365168}
}

@article{Hofstadter1995ineradicableEliza,
  title = {The ineradicable Eliza effect and its dangers},
  author = {Douglas Hofstadter},
  year = {1995},
  journal = {Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought}
}

@article{Dillon2020Elizaeffect,
  title = {The Eliza effect and its dangers: from demystification to gender critique},
  author = {Sarah Dillon},
  year = {2020},
  journal = {Journal for Cultural Research},
  volume = {24},
  number = {1},
  pages = {1-15},
  doi = {10.1080/14797585.2020.1754642},
  url = {https://www.tandfonline.com/doi/full/10.1080/14797585.2020.1754642}
}

@article{SubmittedVersion,
  title = {Submitted Version},
  url = {https://www.repository.cam.ac.uk/bitstreams/5a120643-f24b-42d0-b7c6-7c475de62348/download}
}

@article{MageeHegelsLast,
  title = {Hegel's Last Computer},
  author = {Magee, Liam}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{Lacan2011seminarJacques,
  title = {The seminar of Jacques Lacan: Book XVI: From an Other to the other: 1968-1969.},
  author = {Jacques Lacan},
  year = {2011},
  url = {https://esource.dbs.ie/bitstreams/c4e61ea4-adbd-4a00-876c-f7ee6b13ae93/download}
}

@article{Derrida1981Dissemination,
  title = {Dissemination},
  author = {Jacques Derrida},
  year = {1981},
  url = {https://philpapers.org/rec/JOHD-10}
}

@book{Toscano2023LateFascism,
  title = {Late Fascism: Race, Capitalism and the Politics of Crisis},
  author = {Alberto Toscano},
  year = {2023},
  publisher = {Verso Books},
  url = {https://books.google.com.au/books?hl=en&lr=&id=QbC_EAAAQBAJ&oi=fnd&pg=PR9&dq=toscano+late+fascism&ots=fqxJVKygzY&sig=YJV43bUhyBBjD-LUCMwWPNN3fBY}
}

@article{Magee2023Structuredlike,
  title = {Structured like a language model: Analysing AI as an automated subject},
  author = {Liam Magee, Vanicka Arora, Luke Munn},
  year = {2023},
  journal = {Big Data & Society},
  volume = {10},
  number = {2},
  pages = {20539517231210273},
  doi = {10.1177/20539517231210273},
  url = {http://journals.sagepub.com/doi/10.1177/20539517231210273},
  abstract = {Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as ‘automated subjects’. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise ‘state-of-the-art’ natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer , its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems.}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://journals.sagepub.com/doi/pdf/10.1177/20539517231210273}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@inproceedings{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@article{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Language Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {http://arxiv.org/abs/2407.18219},
  abstract = {A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.}
}

@inproceedings{QuRecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@inproceedings{Qu2024Recursiveintrospection,
  title = {Recursive introspection: Teaching LLM agents how to self-improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=UPoQqreegH}
}

@article{Preprint,
  title = {Preprint PDF},
  url = {http://arxiv.org/pdf/2407.18219v2}
}

@article{Snapshot,
  title = {Snapshot},
  url = {http://arxiv.org/abs/2407.18219}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=UPoQqreegH}
}

@book{Daub2020Whattech,
  title = {What tech calls thinking: An inquiry into the intellectual bedrock of Silicon Valley},
  author = {Adrian Daub},
  year = {2020},
  publisher = {FSG Originals},
  url = {https://books.google.com.au/books?hl=en&lr=&id=IzDQDwAAQBAJ&oi=fnd&pg=PT86&dq=adrian+daub&ots=bVb23NLOZt&sig=pgyW9WDOH_ZkpLWbmqXFqxUp7jo}
}

@article{Linnemann2024ArtificialIntelligence,
  title = {Artificial Intelligence as a New Field of Activity for Applied Social Psychology–A Reasoning for Broadening the Scope},
  author = {Gesa Alena Linnemann, Linda-Elisabeth Reimann},
  year = {2024},
  url = {https://www.preprints.org/manuscript/202406.1586}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://www.preprints.org/manuscript/202406.1586/download/final_file}
}

@article{Weizenbaum1966ELIZAacomputer,
  title = {ELIZA—a computer program for the study of natural language communication between man and machine},
  author = {Joseph Weizenbaum},
  year = {1966},
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36-45},
  doi = {10.1145/365153.365168},
  url = {https://dl.acm.org/doi/10.1145/365153.365168}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://dl.acm.org/doi/pdf/10.1145/365153.365168}
}

@article{Hofstadter1995ineradicableEliza,
  title = {The ineradicable Eliza effect and its dangers},
  author = {Douglas Hofstadter},
  year = {1995},
  journal = {Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought}
}

@article{Dillon2020Elizaeffect,
  title = {The Eliza effect and its dangers: from demystification to gender critique},
  author = {Sarah Dillon},
  year = {2020},
  journal = {Journal for Cultural Research},
  volume = {24},
  number = {1},
  pages = {1-15},
  doi = {10.1080/14797585.2020.1754642},
  url = {https://www.tandfonline.com/doi/full/10.1080/14797585.2020.1754642}
}

@article{SubmittedVersion,
  title = {Submitted Version},
  url = {https://www.repository.cam.ac.uk/bitstreams/5a120643-f24b-42d0-b7c6-7c475de62348/download}
}

@article{Snapshot,
  title = {Snapshot},
  url = {https://responsiblestatecraft.org/peter-thiel-israel-palantir/}
}

@misc{PeterThiel,
  title = {Peter Thiel: 'I defer to Israel' | Responsible Statecraft},
  url = {https://responsiblestatecraft.org/peter-thiel-israel-palantir/},
  abstract = {Video surfaces showing the Palantir tech giant struggling to answer question about client's use of AI-generated kill lists}
}

@article{SubmittedVersion,
  title = {Submitted Version},
  url = {https://www.repository.cam.ac.uk/bitstreams/5a120643-f24b-42d0-b7c6-7c475de62348/download}
}

@article{Dillon2020Elizaeffect,
  title = {The Eliza effect and its dangers: from demystification to gender critique},
  author = {Sarah Dillon},
  year = {2020},
  journal = {Journal for Cultural Research},
  volume = {24},
  number = {1},
  pages = {1-15},
  doi = {10.1080/14797585.2020.1754642},
  url = {https://www.tandfonline.com/doi/full/10.1080/14797585.2020.1754642}
}

@article{Hofstadter1995ineradicableEliza,
  title = {The ineradicable Eliza effect and its dangers},
  author = {Douglas Hofstadter},
  year = {1995},
  journal = {Fluid concepts and creative analogies: Computer models of the fundamental mechanisms of thought}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://dl.acm.org/doi/pdf/10.1145/365153.365168}
}

@article{Weizenbaum1966ELIZAacomputer,
  title = {ELIZA—a computer program for the study of natural language communication between man and machine},
  author = {Joseph Weizenbaum},
  year = {1966},
  journal = {Communications of the ACM},
  volume = {9},
  number = {1},
  pages = {36-45},
  doi = {10.1145/365153.365168},
  url = {https://dl.acm.org/doi/10.1145/365153.365168}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://www.preprints.org/manuscript/202406.1586/download/final_file}
}

@article{Linnemann2024ArtificialIntelligence,
  title = {Artificial Intelligence as a New Field of Activity for Applied Social Psychology–A Reasoning for Broadening the Scope},
  author = {Gesa Alena Linnemann, Linda-Elisabeth Reimann},
  year = {2024},
  url = {https://www.preprints.org/manuscript/202406.1586}
}

@book{Vandiver2024HardMen,
  title = {Hard Men, Hard Money, Hardening Right: Bitcoin, Peter Thiel, and Schmittian States of Exception},
  author = {Josh Vandiver},
  year = {2024},
  pages = {205–230},
  publisher = {Routledge},
  url = {https://www.taylorfrancis.com/chapters/edit/10.4324/9781003436737-16/hard-men-hard-money-hardening-right-josh-vandiver}
}

@book{Daub2020Whattech,
  title = {What tech calls thinking: An inquiry into the intellectual bedrock of Silicon Valley},
  author = {Adrian Daub},
  year = {2020},
  publisher = {FSG Originals},
  url = {https://books.google.com.au/books?hl=en&lr=&id=IzDQDwAAQBAJ&oi=fnd&pg=PT86&dq=adrian+daub&ots=bVb23NLOZt&sig=pgyW9WDOH_ZkpLWbmqXFqxUp7jo}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=UPoQqreegH}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@article{Snapshot,
  title = {Snapshot},
  url = {http://arxiv.org/abs/2407.18219}
}

@article{Preprint,
  title = {Preprint PDF},
  url = {http://arxiv.org/pdf/2407.18219v2}
}

@inproceedings{Qu2024Recursiveintrospection,
  title = {Recursive introspection: Teaching LLM agents how to self-improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=UPoQqreegH}
}

@inproceedings{QuRecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@article{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Language Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {http://arxiv.org/abs/2407.18219},
  abstract = {A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.}
}

@inproceedings{Qu2024RecursiveIntrospection,
  title = {Recursive Introspection: Teaching Foundation Model Agents How to Self-Improve},
  author = {Yuxiao Qu, Tianjun Zhang, Naman Garg, Aviral Kumar},
  year = {2024},
  url = {https://openreview.net/forum?id=qDXdmdBLhR}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://openreview.net/pdf?id=qDXdmdBLhR}
}

@article{AvailableVersion,
  title = {Available Version (via Google Scholar)},
  url = {https://journals.sagepub.com/doi/pdf/10.1177/20539517231210273}
}

@article{Magee2023Structuredlike,
  title = {Structured like a language model: Analysing AI as an automated subject},
  author = {Liam Magee, Vanicka Arora, Luke Munn},
  year = {2023},
  journal = {Big Data & Society},
  volume = {10},
  number = {2},
  pages = {20539517231210273},
  doi = {10.1177/20539517231210273},
  url = {http://journals.sagepub.com/doi/10.1177/20539517231210273},
  abstract = {Drawing from the resources of psychoanalysis and critical media studies, in this article we develop an analysis of large language models (LLMs) as ‘automated subjects’. We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which artificial intelligence (AI) behaviour, including its productions of bias and harm, can be analysed. First, we introduce language models, discuss their significance and risks, and outline our case for interpreting model design and outputs with support from psychoanalytic concepts. We trace a brief history of language models, culminating with the releases, in 2022, of systems that realise ‘state-of-the-art’ natural language processing performance. We engage with one such system, OpenAI's InstructGPT, as a case study, detailing the layers of its construction and conducting exploratory and semi-structured interviews with chatbots. These interviews probe the model's moral imperatives to be ‘helpful’, ‘truthful’ and ‘harmless’ by design. The model acts, we argue, as the condensation of often competing social desires, articulated through the internet and harvested into training data, which must then be regulated and repressed. This foundational structure can however be redirected via prompting, so that the model comes to identify with, and transfer , its commitments to the immediate human subject before it. In turn, these automated productions of language can lead to the human subject projecting agency upon the model, effecting occasionally further forms of countertransference. We conclude that critical media methods and psychoanalytic theory together offer a productive frame for grasping the powerful new capacities of AI-driven language systems.}
}

@book{Toscano2023LateFascism,
  title = {Late Fascism: Race, Capitalism and the Politics of Crisis},
  author = {Alberto Toscano},
  year = {2023},
  publisher = {Verso Books},
  url = {https://books.google.com.au/books?hl=en&lr=&id=QbC_EAAAQBAJ&oi=fnd&pg=PR9&dq=toscano+late+fascism&ots=fqxJVKygzY&sig=YJV43bUhyBBjD-LUCMwWPNN3fBY}
}

@article{Derrida1981Dissemination,
  title = {Dissemination},
  author = {Jacques Derrida},
  year = {1981},
  url = {https://philpapers.org/rec/JOHD-10}
}

@article{Lacan2011seminarJacques,
  title = {The seminar of Jacques Lacan: Book XVI: From an Other to the other: 1968-1969.},
  author = {Jacques Lacan},
  year = {2011},
  url = {https://esource.dbs.ie/bitstreams/c4e61ea4-adbd-4a00-876c-f7ee6b13ae93/download}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{Mageetest,
  title = {test},
  author = {Magee, L.}
}

@article{MageeHegelsLast,
  title = {Hegel's Last Computer},
  author = {Magee, Liam}
}